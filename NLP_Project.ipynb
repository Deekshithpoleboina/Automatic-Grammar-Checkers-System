{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9f16e1d"
   },
   "source": [
    "# üìù Automatic Grammar Checker System\n",
    "This project uses the T5 Transformer model to automatically correct grammatical errors in text. The system is trained on merged and preprocessed grammar datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ba591e"
   },
   "source": [
    "## üì¶ Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69d00ce9"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers datasets torch evaluate rouge_score scikit-learn matplotlib seaborn --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27f16b2f"
   },
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8553632a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fe51086"
   },
   "source": [
    "## üìÇ Load and Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "9604fb04",
    "outputId": "52d5f17e-d124-4427-f6c8-44fe8906c318"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Now load them normally\n",
    "import json\n",
    "\n",
    "with open(\"dataset1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open(\"dataset2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "# Merge them\n",
    "samples = []\n",
    "for item in data1 + data2:\n",
    "    if \"original\" in item and \"corrected\" in item:\n",
    "        samples.append({\n",
    "            \"input\": \"grammar: \" + item[\"original\"],\n",
    "            \"target\": item[\"corrected\"]\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(samples)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b256d91"
   },
   "source": [
    "## ‚úÇÔ∏è Split and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "8d31664b69d046f99b63b0b4e8329bd7",
      "816b00bdf6f64d1bacfdc9d398a39ade",
      "4adf2ee343c84ea09ec494fc9c0124dc",
      "bc7d300bd3364a3dba430f63afc56199",
      "42c2c7715f504bd9ba689ad73f9d84bc",
      "24d84cc286b047068c3ee281c5b031fe",
      "b0042c97ef8246f2bba6ce8dc10d8e99",
      "0df09a63952143a8bd308ac674cd9cfa",
      "0c2dd53d087d4caab66470704b848d46",
      "4279762bba074354a2ad95545a26d82f",
      "a256b236641b43b8a9fc605f947c6ad2",
      "81bc76dd1b7346ac8ebeac6995eee98b",
      "d574a1dc53994bc1b8335ed5d57d38d1",
      "1e0ef6b44b1544c6bdf415ad74984008",
      "ba295181115f42bc82e0c8be12174b0f",
      "20b1358106de40018fd7ccf46d7a5574",
      "4d69d27173db469d9f6a1a6133320602",
      "37519a6f5726412eb38ce2acf9f81c47",
      "cbf857f6d8b6406e9fd5c0fd19766db4",
      "a5a1f4c8de794bd3a083a8c895629eab",
      "a86f66026847482095fc87d1cbe9245c",
      "eb4c8bfc690740ae9202e7f2e7b5fe7f"
     ]
    },
    "id": "0bb56a4a",
    "outputId": "12eebc22-bd93-46d1-dada-87ab43e13d7b"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples[\"target\"], padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "dataset = Dataset.from_list(samples)\n",
    "splits = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = splits[\"train\"].map(preprocess_function, batched=True)\n",
    "test_dataset = splits[\"test\"].map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e712cfde"
   },
   "source": [
    "## üì• Load Pretrained T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fccf5847"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fabea966"
   },
   "source": [
    "## üèãÔ∏è Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Epk7YvCupgSu"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./grammar-checker\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff285cd2"
   },
   "source": [
    "## üöÇ Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "19cdfb71",
    "outputId": "198f2563-50d9-4d0f-b5b1-05e360651f30"
   },
   "outputs": [],
   "source": [
    "training_result = trainer.train()\n",
    "train_loss = training_result.training_loss\n",
    "print(f\"‚úÖ Training Complete with final loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlUC7Azsn9WE"
   },
   "source": [
    "üìà Plot: Training Loss Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea-Tc1U2n7va"
   },
   "outputs": [],
   "source": [
    "def plot_loss_curve(log_history):\n",
    "    train_loss = [log['loss'] for log in log_history if 'loss' in log]\n",
    "    eval_loss = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "    steps = range(0, len(train_loss) * 100, 100)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(steps, train_loss, label='Training Loss')\n",
    "    if eval_loss:\n",
    "        plt.plot(steps, eval_loss, label='Evaluation Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Evaluation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_curve(trainer.state.log_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef76db6d"
   },
   "source": [
    "## üìä Evaluation: BLEU, ROUGE, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a248bb4a",
    "outputId": "1f01abb3-b238-4e81-acbb-132a364703da"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import gc\n",
    "\n",
    "# Move model to CPU for evaluation\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load BLEU and ROUGE\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Safe manual prediction\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(test_dataset.select(range(0, 100))):  # only 100 safe\n",
    "    input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "    input_text = input_text.replace(\"grammar: \", \"\")\n",
    "\n",
    "    # Manually predict\n",
    "    inputs = tokenizer(\"grammar: \" + input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    output_ids = model.generate(inputs[\"input_ids\"].to(device))\n",
    "    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    label = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "    references.append([label])  # List inside list for BLEU\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Now compute BLEU and ROUGE safely\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "rouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "\n",
    "print(f\"üîπ BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"üîπ ROUGE-L Score: {rouge_score['rougeL']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbtpCW3voX-n"
   },
   "source": [
    "  Plot: BLEU & ROUGE Score Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "ELitJFbeoW5Q",
    "outputId": "acfd2731-be97-46a5-d50d-37892faca305"
   },
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'BLEU': bleu_score['bleu'],\n",
    "    'ROUGE-L': rouge_score['rougeL'],\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=list(scores.keys()), y=list(scores.values()), palette='viridis')\n",
    "plt.title('Evaluation Scores')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "721c35da"
   },
   "source": [
    "## ‚úÖ Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UrBz8Z6ojXZ"
   },
   "source": [
    "Plot: Confusion Matrix (Top Samples Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "7089943c",
    "outputId": "be90ad95-a9d5-4405-d20f-9070161d7873"
   },
   "outputs": [],
   "source": [
    "# Take first 100 samples for confusion matrix\n",
    "true = np.array([r[0] for r in references][:100])  # from manual loop\n",
    "pred = np.array(predictions[:100])\n",
    "\n",
    "# Simple label: Correct (match) or Incorrect (mismatch)\n",
    "y_true = (true == true).astype(int)    # 1 if correct\n",
    "y_pred = (true == pred).astype(int)    # 1 if prediction matches\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Simple Match)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04dca5f7"
   },
   "source": [
    "## ‚ú® Test Inference (Grammar Correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7abeaf65"
   },
   "outputs": [],
   "source": [
    "def correct_grammar(sentence):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\"grammar: \" + sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f6062e7"
   },
   "source": [
    "## üìà Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0018f4a"
   },
   "outputs": [],
   "source": [
    "def predict_with_confidence(sentence):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\"grammar: \" + sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "    outputs = model.generate(**inputs, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "    scores = torch.stack(outputs.scores, dim=1)\n",
    "    probs = F.softmax(scores, dim=-1)\n",
    "    max_probs, _ = torch.max(probs, dim=-1)\n",
    "    avg_confidence = torch.mean(max_probs).item()\n",
    "\n",
    "    corrected = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    return corrected, avg_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf7N-SQgotgm",
    "outputId": "e0fe4b9c-e4ff-4804-9593-83d0f55b79f7"
   },
   "outputs": [],
   "source": [
    "example_sentence = \"she go to school everyday\"\n",
    "corrected_sentence, confidence = predict_with_confidence(example_sentence)\n",
    "\n",
    "print(f\"\\nüîπ Original: {example_sentence}\")\n",
    "print(f\"üîπ Corrected: {corrected_sentence}\")\n",
    "print(f\"üîπ Confidence Score: {confidence:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
